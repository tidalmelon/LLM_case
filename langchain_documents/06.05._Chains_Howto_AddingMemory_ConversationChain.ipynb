{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4008cf1-eaac-4e29-8002-c2af13db465e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e5717bb-9614-4767-88da-80cc6170accc",
   "metadata": {},
   "source": [
    "# ConversationChain\n",
    "> **Adding memory (state)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60859796-002b-4d28-8145-b06e0d291537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f668656-b0b8-4563-8e06-3ac7102eade1",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b26448-7e38-48d5-ae88-13d73daf2c48",
   "metadata": {},
   "source": [
    "> Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.<br>\n",
    "> 在calls之间持久化数据, 这将使chain变成有状态的. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b22d47f-9bdf-4a47-aef8-bfab23772bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a903329-d792-4943-a976-2a9ca6978247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2e934-71d6-4ffa-9ee9-79c0b93b5d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32ae39-7215-466e-aa8d-dec702370ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7cc4475-4f11-4db8-a10d-36b0888924cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ee818-cf30-44de-89f4-5c8829b25b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d296a7-80e7-4aea-977c-e3320a3a3c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=chat, memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c01992-280d-485a-bcfb-eb63ba0cf831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21489567-8a4b-4d37-b6e2-4673f9a08c41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first three colors of a rainbow are red, orange, and yellow.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627753cc-5f49-4d26-b0ec-7bc5449ac8aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'And the next 4?',\n",
       " 'history': 'Human: Answer briefly. What are the first 3 colors of a rainbow?\\nAI: The first three colors of a rainbow are red, orange, and yellow.',\n",
       " 'response': 'The next four colors of a rainbow are green, blue, indigo, and violet.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a89e1-c5f9-49af-8a5e-8be524eb0f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e57d196d-36bc-4487-9df2-2486d2eb5e2f",
   "metadata": {},
   "source": [
    "> Essentially, `BaseMemory defines an interface of how langchain stores memory <br>\n",
    "> It allows reading of stored data through load_memory_variables method and storing new data through save_context method. <br>\n",
    "\n",
    "* 通过`BaseMemory.load_memory_variables` 接口存储数据<br>\n",
    "* 通过`BaseMemory.save_context` 存储新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c9abd-2690-490a-9a61-de7c2766b307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "070595a2-f1d7-444c-afd8-9f7426260fc2",
   "metadata": {},
   "source": [
    "## Source Code parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c7088-c372-42f7-a82c-c9de1277e739",
   "metadata": {},
   "source": [
    "> `ConversationChain`是`LLMChain`的子类\n",
    "```python\n",
    "class ConversationChain(LLMChain):\n",
    "    \"\"\"Chain to have a conversation and load context from memory.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain import ConversationChain, OpenAI\n",
    "            conversation = ConversationChain(llm=OpenAI())\n",
    "    \"\"\"\n",
    "\n",
    "    memory: BaseMemory = Field(default_factory=ConversationBufferMemory)\n",
    "    \"\"\"Default memory store.\"\"\"\n",
    "    >> PROMPT 见下文\n",
    "    prompt: BasePromptTemplate = PROMPT\n",
    "    \"\"\"Default conversation prompt to use.\"\"\"\n",
    "    >> input_key 的默认值是 'input'，它在 input_keys(self) 方法中被使用。\n",
    "    input_key: str = \"input\"  #: :meta private:\n",
    "    >> output_key 则被用在 output_keys 方法中，用于指定输出字典的 key。\n",
    "    output_key: str = \"response\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "    >> input_keys 属性会在 pre_inputs 方法中被调用\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Use this since so some prompt vars come from history.\"\"\"\n",
    "        return [self.input_key]\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_prompt_input_variables(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that prompt input variables are consistent.\"\"\"\n",
    "        memory_keys = values[\"memory\"].memory_variables\n",
    "        input_key = values[\"input_key\"]\n",
    "        >> 输入键不会与内存键重叠\n",
    "        if input_key in memory_keys:\n",
    "            raise ValueError(\n",
    "                f\"The input key {input_key} was also found in the memory keys \"\n",
    "                f\"({memory_keys}) - please provide keys that don't overlap.\"\n",
    "            )\n",
    "        prompt_variables = values[\"prompt\"].input_variables\n",
    "        expected_keys = memory_keys + [input_key]\n",
    "        >> 内存键+输入键  与 prompt提示变量 必须一致, 否则抛异常\n",
    "        if set(expected_keys) != set(prompt_variables):\n",
    "            raise ValueError(\n",
    "                \"Got unexpected prompt input variables. The prompt expects \"\n",
    "                f\"{prompt_variables}, but got {memory_keys} as inputs from \"\n",
    "                f\"memory, and {input_key} as the normal input key.\"\n",
    "            )\n",
    "        return values\n",
    "\n",
    " \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d771a25e-c39a-45aa-adc1-42957cf1e3de",
   "metadata": {},
   "source": [
    "默认的**prompt** 接收两个键值对参数：**history** 和 **input**:\n",
    "\n",
    "* input 是当前要处理的数据或请求。这可能是用户输入的一段文本，也可能是其他类型的数据，取决于实际应用的需求。\n",
    "\n",
    "* history 代表了过去状态或行为的记忆，它是由 memory 对象提供的。memory 通常包含一系列先前的事件或交互，用来帮助模型生成响应或执行某些操作。<br>\n",
    "  <font color=blue>例如，在一个聊天机器人应用中，history 可能包括以前的对话历史，以便生成上下文相关的回答。</font><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64fb40-68e6-480e-b852-3695cc4f151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0973b6-2a87-4227-a572-a6e9fdb3747d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**通过这种方式，prompt 对象得以将过去的历史信息（history）和当前的输入信息（input）结合起来，生成针对特定上下文的语言模型查询。**\n",
    "\n",
    "<font color=blue>见上文,这个是ConversationChain的默认提示模板</font>\n",
    "\n",
    "```python\n",
    "DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "# 输入变量是 history 和 当前对话的输入\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=DEFAULT_TEMPLATE)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4e749-59d7-4426-a35a-2c66f3810040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9cf80bd-7a1f-46cd-9273-e67bcd3d06ae",
   "metadata": {},
   "source": [
    "ConversationChain执行流程:<br>\n",
    "\n",
    "1. 在 `pre_inputs`过程中，`memory` 对象会被加载，并与 `input` 变量共同构成 `prompt` 的输入。\n",
    "2. 这个输入被传递给 prompt 模板，执行 `prep_prompts` 方法生成 `PromptValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a5194-47cb-4816-a321-48872ef096c8",
   "metadata": {},
   "source": [
    "`prep_prompts`是基类`LLMChain`的方法\n",
    "\n",
    "```python\n",
    "def prep_prompts(\n",
    "    self,\n",
    "    input_list: List[Dict[str, Any]],\n",
    "    run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    ") -> Tuple[List[PromptValue], Optional[List[str]]]:\n",
    "    \"\"\"Prepare prompts from inputs.\"\"\"\n",
    "    stop = None\n",
    "    if \"stop\" in input_list[0]:\n",
    "        stop = input_list[0][\"stop\"]\n",
    "    prompts = []\n",
    "    for inputs in input_list:\n",
    "        selected_inputs = {k: inputs[k] for k in self.prompt.input_variables}\n",
    "        prompt = self.prompt.format_prompt(**selected_inputs)\n",
    "        _colored_text = get_colored_text(prompt.to_string(), \"green\")\n",
    "        _text = \"Prompt after formatting:\\n\" + _colored_text\n",
    "        if run_manager:\n",
    "            run_manager.on_text(_text, end=\"\\n\", verbose=self.verbose)\n",
    "        if \"stop\" in inputs and inputs[\"stop\"] != stop:\n",
    "            raise ValueError(\n",
    "                \"If `stop` is present in any inputs, should be present in all.\"\n",
    "            )\n",
    "        prompts.append(prompt)\n",
    "    return prompts, stop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959efa4-c981-40f2-b964-9cd1a8050119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b3f71-525d-41bd-82f4-da35602bdd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb0792-639d-4429-94b7-9a473a96ac96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef4d44-68a0-4f6a-96e4-97d2a1206d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e3710-15f9-4c6c-8cb1-a00a985751f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488a4a4-3c5b-4a41-aad6-68cb49fec116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1661cd-c151-41b5-a1b5-9879a5831350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2355d9-080d-49a6-b2db-4d7158c69cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94359e16-aa32-4709-9ef8-b6daade1a188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f797d54-a0df-427e-8062-1663bcb2f1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d333c-af9f-4555-b388-abb441c8f175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5770485-3fa7-4af4-a867-8bc85d894859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a0ad7-ea59-4356-ad92-e308013dda1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2676e-1050-4446-bfe0-41b7b471f6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f05627-93d1-4531-ba5f-c73cee6de47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441766b-f3ae-4fa7-8c91-a2ef7fbff9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35554e48-8dbe-4000-bab4-0385aab219de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc676a-1623-4b49-b7c4-14e46603070e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18e316-8f7d-4a34-a000-f1392627bb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337e20d-cf5f-4eb3-93a7-ff2a950b7f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb48dce-1309-4211-8513-d6dfa5c3bc06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd50ef8-295c-4f54-be38-3a0cd54f9428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1b532-3c88-4eb0-aaab-7218a6b890b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d26987-7d87-46ee-a5cc-d8b486b5b393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13b541-92e5-4421-96ea-b741a65f61cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-preventloss-py",
   "name": "pytorch-gpu.1-13.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m105"
  },
  "kernelspec": {
   "display_name": "chatglm26b",
   "language": "python",
   "name": "chatglm26b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
