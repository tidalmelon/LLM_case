{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfd9c68-5f75-458a-b666-a023149511b7",
   "metadata": {},
   "source": [
    "# Faiss\n",
    "> **Facebook AI Similarity Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c46a9-5a97-4380-9e19-16aaa4dc937b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2527b-9c3b-4744-b0db-8115b7e58592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "307078e6-0fb0-483b-a27b-66580e87bfe8",
   "metadata": {},
   "source": [
    "## synchronous \n",
    "\n",
    "\n",
    "**[ASYNC版本](https://python.langchain.com/docs/integrations/vectorstores/async_faiss)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3653aa61-982f-418d-a25d-5d93d8eef6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d379c9-6ed9-4c6a-8b75-ca485a09dfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce17ca50-ac3f-4429-8123-5b655f173a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = TextLoader('./input/state_of_the_union.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512fc351-a2e3-4b1d-b68f-24bea0e92fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d75656e-fd55-4f45-8cd4-8fb86ae0a4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2677d85-22e2-4901-b9d3-696904eb6c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2181eaae-41bc-43c6-8ad3-55977e374f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541e510e-071d-4f22-9772-85288f5f6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006f7e4-5e06-40dd-a793-71fd08e9c150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af08beb-ab82-4ce1-bf2f-b13f5a0f97c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f60a5ac-c817-493f-a731-9da2b8fb0f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b86d71-939d-44b4-9fd7-2d9f6f5883d2",
   "metadata": {},
   "source": [
    "### 1. 字符串查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f8d2b2-0389-44bc-a503-ab8e8cf8cfee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b41dc06-aca8-4a26-a41a-6defbd7f42c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99db17f7-8662-4bbb-8ffd-294b996cabfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689c0ffb-264a-43f6-9619-9b9c61e6138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45cb32-d670-46c1-8787-889b6c57ad1b",
   "metadata": {},
   "source": [
    "### 2. 字符串查,且返回分数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4601e8-3753-48e6-b4a0-0ce5fefd93eb",
   "metadata": {},
   "source": [
    "`similarity_search_with_score`\n",
    "1. L2 distance : a lower score is better\n",
    "2. 不仅返回文档,还返回score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f05adb-9839-4d23-868e-f6bc7725fb53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_and_scores = db.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d91006a-c2e8-44c5-97a3-99095243a906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.' metadata={'source': './input/state_of_the_union.txt'} 0.36921751\n",
      "page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.' metadata={'source': './input/state_of_the_union.txt'} 0.43448514\n",
      "page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic.' metadata={'source': './input/state_of_the_union.txt'} 0.4524229\n",
      "page_content='Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.' metadata={'source': './input/state_of_the_union.txt'} 0.45298362\n"
     ]
    }
   ],
   "source": [
    "for doc, score in docs_and_scores:\n",
    "    print(doc, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec496b-5896-46e4-baec-717e110ea7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68f0327f-fab3-4974-9253-c39699873a41",
   "metadata": {},
   "source": [
    "### 3. 根据向量查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "028785b3-5a90-4eab-a072-d0ca9a19707b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_vector = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e96d78ed-ff32-4999-9ba6-d20749c10a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.' metadata={'source': './input/state_of_the_union.txt'} 0.36912858\n",
      "page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.' metadata={'source': './input/state_of_the_union.txt'} 0.4345453\n",
      "page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic.' metadata={'source': './input/state_of_the_union.txt'} 0.45249158\n",
      "page_content='Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.' metadata={'source': './input/state_of_the_union.txt'} 0.4529583\n"
     ]
    }
   ],
   "source": [
    "for doc, score in db.similarity_search_with_score_by_vector(embedding_vector):\n",
    "    print(doc, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d75cd9-bfec-47ca-af52-49af7abb3db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b45aec-c69b-4461-a1f9-da877317096f",
   "metadata": {},
   "source": [
    "### 4. Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "681eb5b3-d53f-4057-89cb-69c0b42df528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.save_local('./faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef8745d-49c7-4086-b0df-9e2cb158e2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_db = FAISS.load_local('./faiss_index', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a50a4-ba0e-4f8a-a1a6-8ac4346e8844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264340f2-c8b6-4c1d-9532-a4191412e632",
   "metadata": {},
   "source": [
    "### 5. Serializing and De-Serializing to bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d222fd9-ea10-4f09-a3f6-d603f79ac0b7",
   "metadata": {},
   "source": [
    "1. save_local 空间很大, \n",
    "2. 序列化接口很小, 如果要将向量库持久化到sql数据库, 这是个不错的选择. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9d011-e7ff-4f12-9b14-9b45182f5e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdd292-349e-4edb-88b4-a3b428260ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4033d12-69a3-417f-8a23-127c327a7033",
   "metadata": {},
   "source": [
    "**序列化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a18a98-c713-4ed8-968b-f2f4bca0e66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkl = db.serialize_to_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde6b318-3a40-403a-937d-2f79fb4dbd5c",
   "metadata": {},
   "source": [
    "**反序列化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "879691d9-746b-4138-8376-e55487e977a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = FAISS.deserialize_from_bytes(embeddings=embeddings, serialized=pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6201b5-e93f-4518-a549-a64ce4107aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd5b9b3-ab22-46a8-8a34-77dbee180876",
   "metadata": {},
   "source": [
    "### 6. Merging, 合并索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc3e1223-c618-4136-9e6f-0f162485260a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db1 = FAISS.from_texts([\"foo\"], embeddings)\n",
    "db2 = FAISS.from_texts(['bar'], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a663f17-c61f-4f9c-bf55-18c43743d624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1c3c6c3d-d931-44dc-b147-96cddd259275': Document(page_content='foo')}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a53da54a-f8c4-4480-8c8d-d34667e6c1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3d91c136-36bf-462a-9248-14adec957622': Document(page_content='bar')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96a29f-51dd-479b-ae5e-643e20376ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21e6bc7c-16cd-46b0-a99a-960dcdee1226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db1.merge_from(db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74a95ed7-f644-4c40-ab36-ed1a0588d1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1c3c6c3d-d931-44dc-b147-96cddd259275': Document(page_content='foo'),\n",
       " '3d91c136-36bf-462a-9248-14adec957622': Document(page_content='bar')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b2aed-9ec3-48a1-af02-75ee6ae9e03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0513c3a6-6f0d-4ccc-bbf4-cb63ab44a3de",
   "metadata": {},
   "source": [
    "### 7. 带过滤器的相似度查找"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40b509-2e18-4688-acaf-2f5d5351d9d2",
   "metadata": {},
   "source": [
    "> 1. fetch_k : 是过滤之前获取的文档数 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ecf2f7-8e32-429e-882f-fbd2b995af7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d418c3-1399-4f67-a521-c740c071dbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b27846b-c2c8-4581-87de-425c5f64cdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_documents = [\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958144e5-1ac3-457d-bd36-5ff635fc1e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c44d83dc-3a7d-42cd-9c6d-338eda81761e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(list_of_documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c1f238e-d1c0-4024-94e4-2ceb8cbe644f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fb660ac-e976-4ef2-b7f7-e0e3962fd5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}, Score: 0.0\n",
      "Content: foo, Metadata: {'page': 2}, Score: 0.0\n",
      "Content: foo, Metadata: {'page': 3}, Score: 0.0\n",
      "Content: foo, Metadata: {'page': 4}, Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d2c7b-1c19-4282-bf1b-db9e10d05381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c043df4d-cb6a-4200-98af-a42f77c6837b",
   "metadata": {},
   "source": [
    "> Now we make the same query call but we filter for only `page = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aaea8121-b24d-412f-ab5c-93e4e6af03b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}, Score: 1.4206954801920801e-05\n",
      "Content: bar, Metadata: {'page': 1}, Score: 0.3131061792373657\n"
     ]
    }
   ],
   "source": [
    "results_with_scores = db.similarity_search_with_score(\"foo\", filter=dict(page=1))\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c95c6-22c6-421a-8136-74ff4220fea4",
   "metadata": {},
   "source": [
    "> Same thing can be done with the `max_marginal_relevance_search` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55ded93e-42e5-4a8d-9c58-d155340c2362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n",
      "Content: bar, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "results = db.max_marginal_relevance_search(\"foo\", filter=dict(page=1))\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044a83d-1321-41c2-9e61-6e0b97d6aa63",
   "metadata": {},
   "source": [
    "> `fetch_k` parameter is the number of documents that will be fetched before filtering<br>\n",
    "> `fetch_k` parameter 是在过滤之前抓取的文档数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "868089cb-3f56-42a6-834c-5201acca1098",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "results = db.similarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\n",
    "for doc in results:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5c007-ab97-4fe2-8670-95a0850fe59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29504e94-26e4-4284-aae0-5c2cc2d37f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a992967c-761b-4fe6-9f64-5aa7ed88d545',\n",
       " 1: 'ba1094b9-b7c8-437c-a1db-33c1a39fdfe1',\n",
       " 2: 'b31d8eac-3d9c-4d26-8244-2272a14b9122',\n",
       " 3: 'a8dc35f4-6334-42ae-b76d-87fb5a123674',\n",
       " 4: '7a95ca75-8b31-4744-8a7b-f41915b09c6e',\n",
       " 5: 'a3f06aaf-096b-48fe-ba16-36a3da942f65',\n",
       " 6: '9aa83411-5832-41c1-8fff-091e13f2cd5e',\n",
       " 7: '3d8263fd-7c18-444c-b5e4-b3d73b5abf91'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79964cbc-3e29-4d88-9b20-d827b1b0998a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a992967c-761b-4fe6-9f64-5aa7ed88d545'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abed58de-08df-41b5-98a3-40fb2d7ee1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.delete([db.index_to_docstore_id[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1dd5c641-392c-4308-bfdb-ee9365df07f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98f8f64f-ce91-4525-9400-f6c4317d26a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ba1094b9-b7c8-437c-a1db-33c1a39fdfe1',\n",
       " 1: 'b31d8eac-3d9c-4d26-8244-2272a14b9122',\n",
       " 2: 'a8dc35f4-6334-42ae-b76d-87fb5a123674',\n",
       " 3: '7a95ca75-8b31-4744-8a7b-f41915b09c6e',\n",
       " 4: 'a3f06aaf-096b-48fe-ba16-36a3da942f65',\n",
       " 5: '9aa83411-5832-41c1-8fff-091e13f2cd5e',\n",
       " 6: '3d8263fd-7c18-444c-b5e4-b3d73b5abf91'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0be223-2740-4273-a1a0-40ca1838e3b7",
   "metadata": {},
   "source": [
    "> 少了一个 `a992967c-761b-4fe6-9f64-5aa7ed88d545`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adafd1-f655-49ae-b49e-49dd83ad3ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43ac2d-bedb-4efc-baf8-eb1b0ea5f89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51e831d0-4349-4727-b411-a76240a40c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'List[Document]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'List[str]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run more documents through the embeddings and add to the vectorstore.\n",
       "\n",
       "Args:\n",
       "    documents (List[Document]: Documents to add to the vectorstore.\n",
       "\n",
       "Returns:\n",
       "    List[str]: List of IDs of the added texts.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/preventloss/lib/python3.9/site-packages/langchain/schema/vectorstore.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.add_documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d2d4403-24e3-4599-8395-f90fd18ef729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[dict]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'List[str]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run more texts through the embeddings and add to the vectorstore.\n",
       "\n",
       "Args:\n",
       "    texts: Iterable of strings to add to the vectorstore.\n",
       "    metadatas: Optional list of metadatas associated with the texts.\n",
       "    ids: Optional list of unique IDs.\n",
       "\n",
       "Returns:\n",
       "    List of ids from adding the texts into the vectorstore.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/preventloss/lib/python3.9/site-packages/langchain/vectorstores/faiss.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db.add_texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0c4b5-4941-40e3-a3aa-a0fd299222ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464eb83-9d63-495f-9c69-818f526ac24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "447d95c2-7aaf-48ec-b4e3-ad5dc947b86e",
   "metadata": {},
   "source": [
    "# FAISS 源码解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c86dcc-2cd4-43a1-91b2-5fcd0918f926",
   "metadata": {},
   "source": [
    "> 1. langchain 里默认 faiss的索引是IndexFlatL2(欧式距离) , 也支持IndexFlatIP(内积)<br><br>\n",
    "    ```python\n",
    "        FAISS.from_documents(docuemnts=documents, embedding=embedding, distance_strategy=\"MAX_INNER_PRODUCT\")\n",
    "    ```\n",
    "    \n",
    "> 2. Uncomment the following line if you need to initialize FAISS with no AVX2 optimization\n",
    "    ```python\n",
    "        # 如果您需要在没有 AVX2 优化的情况下初始化 FAISS，请取消以下注释\n",
    "        # os.environ['FAISS_NO_AVX2'] = '1'\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4ca3d-1079-49da-80df-db64e971261f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "338a8f37-0fdc-4552-8e46-7d3e942ceed9",
   "metadata": {},
   "source": [
    "## 数据结构\n",
    "\n",
    "![](./imgs/FAISS_data_structure.png)\n",
    "\n",
    "### index:\n",
    "> <font color=blue>这个就是facebook的faiss的向量库</font>\n",
    "\n",
    "### Docstore:\n",
    "> 存储Document唯一id与Document对应关系的字典<br>\n",
    "> <font color=blue>key: id,  str<font><br>\n",
    "> <font color=blue>value: Document</font>\n",
    "\n",
    "### index_to_docstore_id\n",
    "> Dict[int, str] <br>\n",
    "> faiss的自增序列-> Document唯一id<br>  \n",
    "\n",
    "    \n",
    " \n",
    "---\n",
    "### 源码分析过程\n",
    "\n",
    "> 1. search： 输入str, 如果存在返回 page summary + 一个 Document对象\n",
    "> 2. search:  输入str, 如果不存在, 返回相似实体(<font color=blue>似乎并没有实现，InMemoryDocstore，仅仅是返回找不到</font>)\n",
    "> 3. delete:  <font color=blue>从内存字典中， 删除IDs </font>\n",
    "> 4. add: <font color=blue>增加documents Dict[str->Document]</font>\n",
    "\n",
    "```python\n",
    "class Docstore(ABC):                                                                                                                                       \n",
    "    \"\"\"Interface to access to place that stores documents.\"\"\"                                                                                              \n",
    "                                                                                                                                                           \n",
    "    @abstractmethod                                                                                                                                        \n",
    "    def search(self, search: str) -> Union[str, Document]:                                                                                                 \n",
    "        \"\"\"Search for document.                                                                                                                            \n",
    "                                                                                                                                                           \n",
    "        If page exists, return the page summary, and a Document object.                                                                                    \n",
    "        If page does not exist, return similar entries.                                                                                                    \n",
    "        \"\"\"                                                                                                                                                \n",
    "                                                                                                                                                           \n",
    "    def delete(self, ids: List) -> None:                                                                                                                   \n",
    "        \"\"\"Deleting IDs from in memory dictionary.\"\"\"                                                                                                      \n",
    "        raise NotImplementedError  \n",
    "        \n",
    "class AddableMixin(ABC):\n",
    "     \"\"\"Mixin class that supports adding texts.\"\"\"                                                                                                          \n",
    "             \n",
    "     @abstractmethod\n",
    "     def add(self, texts: Dict[str, Document]) -> None:                                                                                                     \n",
    "         \"\"\"Add more documents.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a480daa1-7259-465d-95be-7e8163d5cc8e",
   "metadata": {},
   "source": [
    "```python\n",
    "class InMemoryDocstore(Docstore, AddableMixin):                                                                                                            \n",
    "     \"\"\"Simple in memory docstore in the form of a dict.\"\"\"                                                                                                 \n",
    "                                                                                                                                                            \n",
    "     def __init__(self, _dict: Optional[Dict[str, Document]] = None):                                                                                       \n",
    "         \"\"\"Initialize with dict.\"\"\"                                                                                                                        \n",
    "         self._dict = _dict if _dict is not None else {}                                                                                                    \n",
    "                                                                                                                                                            \n",
    "     def add(self, texts: Dict[str, Document]) -> None:                                                                                                     \n",
    "         \"\"\"Add texts to in memory dictionary.                                                                                                              \n",
    "                                                                                                                                                            \n",
    "         Args:                                                                                                                                              \n",
    "             texts: dictionary of id -> document.                                                                                                           \n",
    "                                                                                                                                                            \n",
    "         Returns:                                                                                                                                           \n",
    "             None                                                                                                                                           \n",
    "         \"\"\"                                                                                                                                                \n",
    "         overlapping = set(texts).intersection(self._dict)                                                                                                  \n",
    "         if overlapping:                                                                                                                                    \n",
    "             raise ValueError(f\"Tried to add ids that already exist: {overlapping}\")                                                                        \n",
    "         self._dict = {**self._dict, **texts}                                                                                                               \n",
    "                                                                                                                                                            \n",
    "     def delete(self, ids: List) -> None:                                                                                                                   \n",
    "         \"\"\"Deleting IDs from in memory dictionary.\"\"\"                                                                                                      \n",
    "         \"\"\"从字典的keys中删除ids\"\"\"\n",
    "         overlapping = set(ids).intersection(self._dict)                                                                                                    \n",
    "         if not overlapping:                                                                                                                                \n",
    "             raise ValueError(f\"Tried to delete ids that does not  exist: {ids}\")                                                                           \n",
    "         for _id in ids:                                                                                                                                                                                                                                                                                                   \n",
    "             self._dict.pop(_id)                                                                                                                            \n",
    "                                                                                                                                                            \n",
    "     def search(self, search: str) -> Union[str, Document]:                                                                                                 \n",
    "         \"\"\"Search via direct lookup.                                                                                                                       \n",
    "                                                                                                                                                            \n",
    "         Args:                                                                                                                                              \n",
    "             search: id of a document to search for.                                                                                                        \n",
    "                                                                                                                                                            \n",
    "         Returns:                                                                                                                                           \n",
    "             Document if found, else error message.                                                                                                         \n",
    "         \"\"\"                                                                                                                                                \n",
    "         if search not in self._dict:                                                                                                                       \n",
    "             return f\"ID {search} not found.\"                                                                                                               \n",
    "         else:                                                                                                                                              \n",
    "             return self._dict[search]            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46718082-c196-4c8b-b23d-a81bb83136fb",
   "metadata": {},
   "source": [
    "> 1. <font color=blue>from_documents 其实是调用的子类的 from_texts 方法 <font>\n",
    "> 2. <font color=blue>默认ids是None, 是一个List\\<str\\><font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728af0e-d1f7-4f96-b592-26dcd677e314",
   "metadata": {},
   "source": [
    "```python\n",
    "class VectorStore(ABC):\n",
    "    \"\"\"Interface for vector store.\"\"\"\n",
    "@classmethod\n",
    "def from_documents(                                                                                                                                                                                                                                                                                                   \n",
    "    cls: Type[VST],\n",
    "    documents: List[Document],\n",
    "    embedding: Embeddings,\n",
    "    **kwargs: Any,\n",
    ") -> VST:\n",
    "    \"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\n",
    "    texts = [d.page_content for d in documents]\n",
    "    metadatas = [d.metadata for d in documents]\n",
    "    return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)\n",
    "\n",
    "\n",
    "class FAISS(VectorStore):     \n",
    "     @classmethod\n",
    "     def from_texts(                                                                                                                                                                                                                                                                                                       \n",
    "         cls,\n",
    "         texts: List[str],\n",
    "         embedding: Embeddings,\n",
    "         metadatas: Optional[List[dict]] = None,\n",
    "         ids: Optional[List[str]] = None,\n",
    "         **kwargs: Any,\n",
    "     ) -> FAISS:\n",
    "         \"\"\"Construct FAISS wrapper from raw documents.\n",
    " \n",
    "         This is a user friendly interface that:\n",
    "             1. Embeds documents.\n",
    "             2. Creates an in memory docstore\n",
    "             3. Initializes the FAISS database\n",
    " \n",
    "         This is intended to be a quick way to get started.\n",
    " \n",
    "         Example:\n",
    "             .. code-block:: python\n",
    " \n",
    "                 from langchain.vectorstores import FAISS\n",
    "                 from langchain.embeddings import OpenAIEmbeddings\n",
    " \n",
    "                 embeddings = OpenAIEmbeddings()\n",
    "                 faiss = FAISS.from_texts(texts, embeddings)\n",
    "         \"\"\"\n",
    "         embeddings = embedding.embed_documents(texts)\n",
    "         return cls.__from(\n",
    "             texts,\n",
    "             embeddings,\n",
    "             embedding,\n",
    "             metadatas=metadatas,\n",
    "             ids=ids,\n",
    "             **kwargs,\n",
    "         )\n",
    "        \n",
    "    @classmethod\n",
    "    def __from(\n",
    "        cls,\n",
    "        texts: Iterable[str],\n",
    "        embeddings: List[List[float]],\n",
    "        embedding: Embeddings,\n",
    "        metadatas: Optional[Iterable[dict]] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "        normalize_L2: bool = False,\n",
    "        distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    "        **kwargs: Any,\n",
    "    ) -> FAISS:\n",
    "        faiss = dependable_faiss_import()\n",
    "        \n",
    "        >> 1. 这里确认了索引类型： 默认是欧式距离（归一化的内积） ， 另一个可选是 内积\n",
    "        >> 1. 这里主要确认了向量的:  宽度\n",
    "        if distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:\n",
    "            index = faiss.IndexFlatIP(len(embeddings[0]))\n",
    "        else:\n",
    "            # Default to L2, currently other metric types not initialized.\n",
    "            index = faiss.IndexFlatL2(len(embeddings[0]))\n",
    "          \n",
    "        >> 2. 实例化: FAISS\n",
    "        >> 2. 默认的文档存储器是: InMemoryDocstore\n",
    "        >> 2. 这里将创建的faiss向量库（无数据）传递给了FAISS类\n",
    "        vecstore = cls(\n",
    "            embedding.embed_query,\n",
    "            index,\n",
    "            InMemoryDocstore(),\n",
    "            {},\n",
    "            normalize_L2=normalize_L2,\n",
    "            #distance_strategy=distance_strategy,\n",
    "            **kwargs,\n",
    "        )\n",
    "        >> 3. 这里是添加文档到 faiss 和 docstore： 存入向量数据库 和 存入 内存文档库\n",
    "        vecstore.__add(texts, embeddings, metadatas=metadatas, ids=ids)\n",
    "        return vecstore\n",
    "    \n",
    "    def __add(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        embeddings: Iterable[List[float]],\n",
    "        metadatas: Optional[Iterable[dict]] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "    ) -> List[str]:\n",
    "        faiss = dependable_faiss_import()\n",
    "    \n",
    "        if not isinstance(self.docstore, AddableMixin):\n",
    "            raise ValueError(\n",
    "                \"If trying to add texts, the underlying docstore should support \"\n",
    "                f\"adding items, which {self.docstore} does not\"\n",
    "            )\n",
    "    \n",
    "        _len_check_if_sized(texts, metadatas, \"texts\", \"metadatas\")\n",
    "        _metadatas = metadatas or ({} for _ in texts)\n",
    "        documents = [\n",
    "            Document(page_content=t, metadata=m) for t, m in zip(texts, _metadatas)\n",
    "        ]\n",
    "    \n",
    "        _len_check_if_sized(documents, embeddings, \"documents\", \"embeddings\")\n",
    "        _len_check_if_sized(documents, ids, \"documents\", \"ids\")\n",
    "    \n",
    "        # Add to the index.\n",
    "        vector = np.array(embeddings, dtype=np.float32)\n",
    "        if self._normalize_L2:\n",
    "            faiss.normalize_L2(vector)\n",
    "        # index是真正的faiss向量数据库， index传过来是空数据库， 这里真正添加了Document的向量-数据\n",
    "        self.index.add(vector)\n",
    "    \n",
    "        >> ids 如果为None，这里去了uuid, 所以docstore本质上是为： 唯一id-> Document的映射\n",
    "        # Add information to docstore and index.\n",
    "        ids = ids or [str(uuid.uuid4()) for _ in texts]\n",
    "        >> docstore本质上是为： 唯一id-> Document的映射\n",
    "        self.docstore.add({id_: doc for id_, doc in zip(ids, documents)})\n",
    "        starting_len = len(self.index_to_docstore_id)\n",
    "        >> faiss.index  与  新增加的ids 之间建立联系 \n",
    "        index_to_id = {starting_len + j: id_ for j, id_ in enumerate(ids)}\n",
    "        self.index_to_docstore_id.update(index_to_id)\n",
    "        return ids\n",
    "    \n",
    "    \n",
    "    def similarity_search_with_score_by_vector(\n",
    "        self,\n",
    "        embedding: List[float],\n",
    "        k: int = 4,\n",
    "        filter: Optional[Dict[str, Any]] = None,\n",
    "        fetch_k: int = 20,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Return docs most similar to query.\n",
    "    \n",
    "        Args:\n",
    "            embedding: Embedding vector to look up documents similar to.\n",
    "            k: Number of Documents to return. Defaults to 4.\n",
    "            filter (Optional[Dict[str, Any]]): Filter by metadata. Defaults to None.\n",
    "            fetch_k: (Optional[int]) Number of Documents to fetch before filtering.\n",
    "                      Defaults to 20.\n",
    "            **kwargs: kwargs to be passed to similarity search. Can include:\n",
    "                score_threshold: Optional, a floating point value between 0 to 1 to\n",
    "                    filter the resulting set of retrieved docs\n",
    "    \n",
    "        Returns:\n",
    "            List of documents most similar to the query text and L2 distance\n",
    "            in float for each. Lower score represents more similarity.\n",
    "        \"\"\"\n",
    "        faiss = dependable_faiss_import()\n",
    "        vector = np.array([embedding], dtype=np.float32)\n",
    "        if self._normalize_L2:\n",
    "            faiss.normalize_L2(vector)\n",
    "        scores, indices = self.index.search(vector, k if filter is None else fetch_k)                                                                                                                                                                                                                                     \n",
    "        docs = []\n",
    "        for j, i in enumerate(indices[0]):\n",
    "            if i == -1:\n",
    "                # This happens when not enough docs are returned.\n",
    "                continue\n",
    "            >> 这里可以验证： 确实是用 index返回的 索引列表 indices 当做key 去查询的  index_to_docstore_id\n",
    "            _id = self.index_to_docstore_id[i]\n",
    "            >> 然后再用返回的 唯一id\n",
    "            doc = self.docstore.search(_id)\n",
    "            if not isinstance(doc, Document):\n",
    "                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\n",
    "            if filter is not None:\n",
    "                filter = {\n",
    "                    key: [value] if not isinstance(value, list) else value\n",
    "                    for key, value in filter.items()\n",
    "                }\n",
    "                if all(doc.metadata.get(key) in value for key, value in filter.items()):\n",
    "                    docs.append((doc, scores[0][j]))\n",
    "            else:\n",
    "                docs.append((doc, scores[0][j]))\n",
    "    \n",
    "        score_threshold = kwargs.get(\"score_threshold\")\n",
    "        if score_threshold is not None:\n",
    "            cmp = (\n",
    "                operator.ge\n",
    "                if self.distance_strategy\n",
    "                in (DistanceStrategy.MAX_INNER_PRODUCT, DistanceStrategy.JACCARD)\n",
    "                else operator.le\n",
    "            )\n",
    "            docs = [\n",
    "                (doc, similarity)\n",
    "                for doc, similarity in docs\n",
    "                if cmp(similarity, score_threshold)\n",
    "            ]\n",
    "        return docs[:k]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6c0c5-cedc-4e06-a74e-28bd19c0e09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad172023-f262-4431-9137-b9dff633df3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddaa427-c58c-4d27-bad9-1e8b074270a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-preventloss-py",
   "name": "pytorch-gpu.1-13.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m105"
  },
  "kernelspec": {
   "display_name": "chatglm26b",
   "language": "python",
   "name": "chatglm26b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
