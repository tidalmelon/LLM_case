{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e851c63-fbb9-4f99-a92b-26dce2e719a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=./imgs/model_io.jpg width=35% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47454cdf-1217-4fbf-a015-5c981b69e2f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "[langchain documents](https://python.langchain.com/docs/modules/model_io/models/chat/llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2806292-4cf0-40eb-9f66-941d282dfab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3137222-ab3d-425b-bc0f-e21575ff9352",
   "metadata": {},
   "source": [
    "> Language Models 分为 LLM 和 Chat Model. <br>\n",
    "> 比如GPT-达芬奇-3 根据 聊天语料 微调后 成为 chatGPT3.5<br>\n",
    "> \"predict\" for LLMs and \"predict messages\" for chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c39cf0-87f6-4bd7-ac78-3ca3648efeab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baba5d0-6a70-43a7-ae40-3b2c5728a2e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6b7cc-13e3-4a84-bb2d-b0bdd4a80738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a5c435-9cce-430d-a325-b53773200ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4c4d2f-76ec-4b2b-88ea-42f879228984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152fd50e-a2f7-4fd2-bd96-620002f52a05",
   "metadata": {},
   "source": [
    "#### string in string out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9d8e3c-5811-4f7d-a474-8d55fcf4f546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54878c55-6e3f-4d5b-a3a4-0b837ac61e45",
   "metadata": {},
   "source": [
    "#### batch call, richer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e0efa9-3255-47f3-80d6-c595c6f6172c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\n",
    "# len(llm_result.generations)\n",
    "# llm_result.generations[0]\n",
    "# llm_result.llm_output\n",
    "# ```\n",
    "# {'token_usage': {'completion_tokens': 3903,\n",
    "#       'total_tokens': 4023,\n",
    "#       'prompt_tokens': 120}}\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78af85-a5b9-4a32-8fd2-75fe5efe87ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb014f57-eb12-4a4d-8d10-1196f4017c55",
   "metadata": {},
   "source": [
    "#### langchain - async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f688be-28ce-4d9f-88e3-702f403ca403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting asyncio\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ead0c-56be-45fb-aad9-8f00392da915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f224c9a-66b5-40e9-9e62-40752519a3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcffc9-3410-496c-b179-5d23251b4bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41de27dc-8a47-486b-87a7-ea2c74820503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    for _ in range(10):\n",
    "        resp = llm.generate(['Hello, how are you?'])\n",
    "        print(resp.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd78dc09-ade5-4e71-8b8e-cbbd1c4e11e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def async_geneate(llm):\n",
    "    resp = await llm.agenerate(['Hello, how are you?'])\n",
    "    print(resp.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e36e9752-76bc-4a05-a47f-aa41c2a5d77c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_geneate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8cf9a19-2d3f-4ec8-b40e-f7888fbbac03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ead56049-5203-4f54-a354-111f6db7935d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s = time.perf_counter()\n",
    "# # If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
    "# await generate_concurrently()\n",
    "# elapsed = time.perf_counter() - s\n",
    "# print(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "\n",
    "# s = time.perf_counter()\n",
    "# generate_serially()\n",
    "# elapsed = time.perf_counter() - s\n",
    "# print(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1d71e-5ab1-43f4-909e-24dc23ad8e2c",
   "metadata": {},
   "source": [
    "> **由于并发限制,这里演示不了**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c8558-13ab-4d60-aa16-d61ad6b3c5b0",
   "metadata": {},
   "source": [
    "#### Custom LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92866eae-819a-4c63-b908-de1d4e1f1eb3",
   "metadata": {},
   "source": [
    "> 定制的LLM包装器(装饰器)wrapper, 如果你要使用自己的私有LLM.<br>\n",
    "> 1. 必须要实现的接口:_call函数,  输入string 一些可选的停用词, 返回一个string<br>\n",
    "> 2. 可选的接口: _identifying_params, 打印此类, 返回一个dictionary<br>\n",
    "\n",
    "> 让我们实现一个简单的LLM,仅 返回输入string的前n个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "631a37a4-60c9-4db6-af3a-d9a215b15085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aedde949-3f4a-4384-92a1-75f20bee8c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    n : int\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553ba21-6321-4833-a35d-598a5732523d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c79740a-ed06-4c35-8123-43a2fa07ed54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = CustomLLM(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05ba8a3c-9b2d-4c97-a2a9-50f8b57dda21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"this is a footbar thing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5d76b-5047-4292-ae4b-db24b82e0aa4",
   "metadata": {},
   "source": [
    "> 看看定制化的print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b9c1be8-b84a-40f4-bbe7-04a4703032af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'n': 10}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce4c60-0144-4f17-b765-12ca722b1205",
   "metadata": {},
   "source": [
    "> _llm_type <br>\n",
    "> n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30496f-e901-4026-a350-d8a357015633",
   "metadata": {},
   "source": [
    "#### Fake LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f858e-6a93-4265-8fa6-a78096597b3d",
   "metadata": {},
   "source": [
    "> 假的LLM, 这个用于测试. 允许你模拟调用LLM, 并模拟返回结果以特定的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "425d97db-a54f-4967-9b68-506ea07cf64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.fake import FakeListLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8113dbe-e965-470a-a6a4-e916b2021491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b5bb8cf-e2d8-4e72-ac86-2397e1d53b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = load_tools(['python_repl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1e94ae1-6863-431d-b519-8ac72c4d1290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = [\"Action: Python REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "llm = FakeListLLM(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f20b270c-9fb1-405e-92fe-c592b049e078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4228a11b-2d77-4406-afe5-ac625846c723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: Python REPL is not a valid tool, try one of [Python_REPL].\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"whats 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a815923-89ba-470f-bf5c-6d364bd2d43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35d86c00-6d0e-44d5-956f-0b26615f60d1",
   "metadata": {},
   "source": [
    "#### Human input LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e59e2-ae8a-4f03-8520-637a3c5ffd19",
   "metadata": {},
   "source": [
    "> 跟Fake LLM类似,langchain提供一个pseudo LLM class 用来测试,debug, 或者教育. <br>\n",
    "> 这个允许你模拟调用LLM并模拟**如果人类收到这个prompt会如何反应**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ade8b62-903e-452c-a0eb-4d4ed06ecf08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.human import HumanInputLLM\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "444f6067-7574-4fc2-ab47-e221c12ac135",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=50930d6572680f6d9e41f1dcdb64fe801a3a9931838264e1c8c21f912aed5f5e\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8988feb-6da6-42cb-adc7-cc91234fa5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = load_tools(['wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a7b28e-bef7-4767-97f6-25508a0cc4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = HumanInputLLM(prompt_func=lambda prompt : print(f\"\\n===PROMPT====\\n{prompt}\\n=====END OF PROMPT======\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ccc0a-0653-4ad7-a77e-beb30b6610b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35f189b-e9f1-4723-bda2-2b7bfd2af806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87b6ec-8707-4eab-a34e-e7b22048a53b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "===PROMPT====\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Wikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Wikipedia]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is 'Bocchi the Rock!'?\n",
      "Thought:\n",
      "=====END OF PROMPT======\n"
     ]
    }
   ],
   "source": [
    "agent.run(\"What is 'Bocchi the Rock!'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c1979-e134-4023-b76f-e71482a7313c",
   "metadata": {},
   "source": [
    "#### Caching\n",
    "\n",
    "> 主打一个省钱:<br>\n",
    "> 1. 减少调用接口的次数<br>\n",
    "> 2. 如果你多次调用一个Completion, 加速并减少调用次数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1432b-3660-4eed-a54d-8c40a1b8b4cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6203a42-f04f-49e8-87a8-3b47aba30756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c1dd6-e813-4aca-9344-10014f3f61e1",
   "metadata": {},
   "source": [
    "> 为了使缓存可见, 我们使用一个低端模型: davici-003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3902319-5870-418a-9ed7-48b380bdfc45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name='text-davinci-002', n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a5357-8fd2-4f05-818e-6c75b49b248d",
   "metadata": {},
   "source": [
    "##### **In Memory Cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f69ab3-b851-4f73-b2d8-9ff93ecac754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e6327e-8050-4a89-9e94-76f8a6f468d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /opt/conda/envs/preventloss/lib/python3.9/site-packages (1.4.49)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.22-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from sqlalchemy) (4.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/preventloss/lib/python3.9/site-packages (from sqlalchemy) (2.0.2)\n",
      "Installing collected packages: sqlalchemy\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.49\n",
      "    Uninstalling SQLAlchemy-1.4.49:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.49\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "feast 0.33.1 requires SQLAlchemy[mypy]<2,>1, but you have sqlalchemy 2.0.22 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sqlalchemy-2.0.22\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba865496-1d00-433b-a533-5d80e53ab0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 0 ns, total: 19 µs\n",
      "Wall time: 28.8 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "# 第一次,并不在缓存, 所以时间比较长\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a258d-d1fc-4823-a997-dc1e09263dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8eb5fa-e855-4036-a1cd-cfaf041a9bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "# 第二次,他就很快了\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04ede3-bae2-4119-8e4c-f8a6cb94d776",
   "metadata": {},
   "source": [
    "##### Sqlite cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a2140f-63a5-4cfd-968c-0e9e24a1c428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '.langchain.db': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm .langchain.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3165d3-17c8-4dc0-850a-edc22044e893",
   "metadata": {},
   "source": [
    "> 我们可以做使用Sqlite cache做相同的事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711a11f6-d7e3-4fb7-8c4b-8591bc096e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path='.langchain.db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75beca81-e433-4c10-a06f-f494be675240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nQ: Why don't scientists trust atoms?\\nA: Because they make up everything\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98216e8f-48d5-4bfc-9840-355e97dcbbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nQ: Why don't scientists trust atoms?\\nA: Because they make up everything\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "# The second time it is, so it goes faster\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c268d2-5d71-4d47-8ebe-94b6698ee119",
   "metadata": {},
   "source": [
    "##### optional caching in chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08c77a7-ab67-4151-95a5-daa4bad001b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name='text-davinci-002')\n",
    "no_cache_llm = OpenAI(model_name='text-davinci-002', cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf85440-385c-47ab-a69f-492bde818c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68cb93fd-455d-447e-887f-d31c2209b550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88561384-e720-49e9-bee8-eef76c525775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c29ed187-1aa8-4e9a-b471-7ca5443b776b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./input/state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164b3657-d30a-4525-8935-b7f6c5df1869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02739b-a6c3-4cb7-b79c-574ea063d895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4860ccf0-8aad-49c2-84a1-0f9c00a49720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=t) for t in texts[:3]]\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2d6424-99b9-4154-a00b-1eace7d82eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type='map_reduce', reduce_llm=no_cache_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84015a50-2647-4d92-b53e-326bb150da73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe United States is working with European allies to respond to Russian aggression in Ukraine. Russian assets will be seized, Russian flights will be banned from American airspace, and military, economic, and humanitarian aid will be provided to Ukraine. These actions are designed to pressure Russia into withdrawing from Ukraine.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30666fe-0de3-42f5-9a50-cb147e7ac08e",
   "metadata": {},
   "source": [
    "> When we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step.\n",
    "\n",
    "> 变快了, 但结果不一样了, 因为我们仅仅缓存了map阶段, 没有缓存reduce阶段, 所以并没有完全\"固话\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63aa05aa-1dd0-4704-85cc-91996bfde5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe United States and its European allies are taking action against Russia in response to Putin's aggression in Ukraine. America will provide military, economic, and humanitarian aid to Ukraine and will pressure Russia until it withdraws from Ukraine.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcc356-b873-43d1-94cf-d1ed3add75a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365fd04b-8e45-4a63-b522-eef6278d5ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'sqlite.db': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rm .langchain.db sqlite.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb1245-3547-406c-86c9-464e3c160052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c5ea737-8953-40c7-bc46-de81c6634c81",
   "metadata": {},
   "source": [
    "#### serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c71e3-b43a-49f6-94f0-9c9d1025ef5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20d0c267-3deb-4c06-b109-3c1c2ae9a0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.llms.loading import load_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32b286d4-2f14-4496-8b1a-ca929a8624e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    {\n",
      "        \"model_name\": \"text-davinci-003\",\n",
      "        \"temperature\": 0.7,\n",
      "        \"max_tokens\": 256,\n",
      "        \"top_p\": 1.0,\n",
      "        \"frequency_penalty\": 0.0,\n",
      "        \"presence_penalty\": 0.0,\n",
      "        \"n\": 1,\n",
      "        \"best_of\": 1,\n",
      "        \"request_timeout\": null,\n",
      "        \"_type\": \"openai\"\n",
      "    }"
     ]
    }
   ],
   "source": [
    "cat ./input/llm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e4596c0-3a59-406d-a897-c830addaf9da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = load_llm('./input/llm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6ac77-a455-4ec9-be69-b0ebc39be375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f352e47-357e-4380-9350-8da65eac268d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    _type: openai\n",
      "    best_of: 1\n",
      "    frequency_penalty: 0.0\n",
      "    max_tokens: 256\n",
      "    model_name: text-davinci-003\n",
      "    n: 1\n",
      "    presence_penalty: 0.0\n",
      "    request_timeout: null\n",
      "    temperature: 0.7\n",
      "    top_p: 1.0"
     ]
    }
   ],
   "source": [
    "cat ./input/llm.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e563d4-1c4e-44ce-b64c-5695f04a1f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7341dc56-fe42-4055-a35d-ee87473d704a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = load_llm('./input/llm.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6cfa0-8b55-4e85-8393-3b048e899406",
   "metadata": {},
   "source": [
    "**saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "639738c7-3ac3-4da5-9b12-df0afd7b461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.save('llm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb588c2e-8809-4eb8-bb56-2b5c50a2629e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c081806-7b74-4783-a5a0-21b6f2579b78",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50379571-a5a6-48b0-b63a-39ab778f552b",
   "metadata": {},
   "source": [
    "一次返回一个字符, 而不是一次性返回.<br>\n",
    "**适合场景**:\n",
    "> 1. 优势: 快速展示<br>\n",
    "> 2. 优势: 每产生一个字符就处理一下."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78424434-cd81-4263-bec7-936171b827ef",
   "metadata": {},
   "source": [
    "**实现方式**\n",
    "> 继承CallbackHandler接口,并实现on_llm_new_token方法."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07517880-61f2-4948-b961-f2a354d9a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "104ef3fe-74c2-4702-b94d-786f3b56ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a71524-bc97-4854-83f6-3a74d505b5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d6872-8772-432c-9668-63fbdebd2f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95862b30-2e7e-47a6-8d61-8e5ea93fb940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2892c514-a089-48f1-97fe-93eeb6cfb5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Verse 1\n",
      "I'm sippin' on sparkling water,\n",
      "It's so refreshing and light,\n",
      "It's the perfect way to quench my thirst\n",
      "On a hot summer night.\n",
      "\n",
      "Chorus\n",
      "Sparkling water, sparkling water,\n",
      "It's the best way to stay hydrated,\n",
      "It's so crisp and so clean,\n",
      "It's the perfect way to stay refreshed.\n",
      "\n",
      "Verse 2\n",
      "I'm sippin' on sparkling water,\n",
      "It's so bubbly and bright,\n",
      "It's the perfect way to cool me down\n",
      "On a hot summer night.\n",
      "\n",
      "Chorus\n",
      "Sparkling water, sparkling water,\n",
      "It's the best way to stay hydrated,\n",
      "It's so crisp and so clean,\n",
      "It's the perfect way to stay refreshed.\n",
      "\n",
      "Verse 3\n",
      "I'm sippin' on sparkling water,\n",
      "It's so light and so clear,\n",
      "It's the perfect way to keep me cool\n",
      "On a hot summer night.\n",
      "\n",
      "Chorus\n",
      "Sparkling water, sparkling water,\n",
      "It's the best way to stay hydrated,\n",
      "It's so crisp and so clean,\n",
      "It's the perfect way to stay refreshed."
     ]
    }
   ],
   "source": [
    "resp = llm(\"Write me a song about sparkling water.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb815c-4114-4f15-97c7-9d0b5f54a5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc5131e-29c8-41de-b660-092d529e126b",
   "metadata": {},
   "source": [
    "> **使用generate 既可以是streaming 也可以获取最终的LLMResult**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0e70243-4bae-4467-b420-ff16248e6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('df8e6732-3bc0-4b0d-b6db-315077417907'))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate([\"Tell me a joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ee9bf-4077-481c-b6c0-f83e82373934",
   "metadata": {},
   "source": [
    "> LLMResult中的output的 **streaming模式 还不支持token_usage: 令牌使用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff341a1-4bfc-4686-9142-7850def45982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef41124a-7e0e-4f4c-b297-66a16fec2635",
   "metadata": {},
   "source": [
    "#### Tracking token usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e184c0-b5b9-4ac5-bc86-c5175334fc8d",
   "metadata": {},
   "source": [
    "> 如何跟踪特定调用的令牌使用情况, 目前只支持OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4213903-a809-4081-b6f6-697ada3e811f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11ebfa7d-1875-4045-8303-dc0f5bf5fcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3e9d9bc-c890-44b7-84cc-c21cd742bdb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4552d8c-3c8c-4a5e-820f-478a1630a2a2",
   "metadata": {},
   "source": [
    "**Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence.**\n",
    "\n",
    "上下文管理器: context manager 内部的调用都会被计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b031bde-929d-499c-bceb-125aeb81f9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    result2 = llm(\"Tell me a joke\")\n",
    "    print(cb.total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a5643-3a12-4e1a-b165-6960b98f956c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12533a84-3d6a-4e67-91e8-0945d19038b9",
   "metadata": {},
   "source": [
    "**If a chain or agent with multiple steps in it is used, it will track all those steps.**\n",
    "\n",
    "**如果一个pipeline 有许多step, 都会被跟踪**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "123a72c3-591b-4639-ab2e-96c46077afb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5674277-9ca6-4e5e-81c8-3f49d9b5e116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1d7f7cf-7793-4c5f-acce-df8539a0bd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "tools = load_tools([\"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bdda5-e6fb-47ee-a07b-c8c6b75d073e",
   "metadata": {},
   "source": [
    "> need serpapi_api_key<br>\n",
    "> llm-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fc13461-fbd1-4e43-82ce-d2a0414e59d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2dcd71d5-5312-4d48-a7f4-cd6ad17531de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with get_openai_callback() as cb:\n",
    "#     response = agent.run(\n",
    "#         \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\n",
    "#     )\n",
    "#     print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "#     print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "#     print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "#     print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195fad7-9f69-4ae1-9e26-dca5fe6de243",
   "metadata": {},
   "source": [
    ">  目前使用的ChatGPT3.5免费的, 所以不展示了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16550fb-1708-42e7-8c6f-f5fbf4cf9663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-preventloss-py",
   "name": "pytorch-gpu.1-13.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m105"
  },
  "kernelspec": {
   "display_name": "chatglm26b",
   "language": "python",
   "name": "chatglm26b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
