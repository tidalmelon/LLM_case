{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e851c63-fbb9-4f99-a92b-26dce2e719a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=./imgs/model_io.jpg width=35% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47454cdf-1217-4fbf-a015-5c981b69e2f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "[langchain documents](https://python.langchain.com/docs/modules/model_io/models/chat/llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2806292-4cf0-40eb-9f66-941d282dfab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b77b43-6e8c-4c03-b08f-ad9c273aaa96",
   "metadata": {},
   "source": [
    "[LangChain-Tutorials](https://github.com/sugarforever/LangChain-Tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53af36f-4eab-4f19-ae0a-350c2649e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f1574-f119-4e8d-bea8-96c9c06b9516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e6f5cf0-ae07-494a-9ff0-dd8ffbfecbff",
   "metadata": {},
   "source": [
    "# Composition 组合整体"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbfe741-7332-4817-b201-691434211458",
   "metadata": {},
   "source": [
    "> This notebook goes over **how to compose multiple prompts together**<br>\n",
    "> This can be useful when you want to **reuse** parts of prompts.<br>\n",
    "> A PipelinePrompt consists of two main parts:<br>\n",
    "> 1. **Final prompt**: This is the final prompt that is returned<br>\n",
    "> 2. **Pipeline prompts**: This is a list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edac844-3cef-4d1f-8c88-dbb9d2ca7a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5a5c58-817b-4a1c-82e4-435c1eaa8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c877999-b5a0-40ac-99a9-4194903577eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7da79f-f454-445d-beae-ff47dacdf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0990af-d45e-4468-8e1d-0099ebbd56e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecd97b-8bf7-4e25-bdc8-e0875ea04bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cfa891-9949-454c-a926-8c1ebcd24d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你扮演某个人\n",
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd2b9c-7053-4e3d-a2ee-788ab87d84f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5066d574-ba15-4d56-a304-a12ba5d19530",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"Here's an example of an interaction: \n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205a68a-8ab6-44ad-aa80-64f94e4bc6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa05b3e-8ca2-44fe-9b07-e76eb58c3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "start_prompt = PromptTemplate.from_template(start_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a9124-9427-4704-b774-edcdd6b806b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc0ff52-8211-4c2a-a85d-c759d0c7f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e9c445-7447-472d-87d9-c988946a2345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example_q', 'person', 'input', 'example_a']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ee8f9-98e6-49ff-8f8b-6af9677a2baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63a047a-5c55-4fd4-9213-f1d6e3f74665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are impersonating Elon Musk.\n",
      "\n",
      "Here's an example of an interaction: \n",
      "\n",
      "Q: What's your favorite car?\n",
      "A: Tesla\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: What's your favorite social media site?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_prompt.format(\n",
    "    person=\"Elon Musk\",\n",
    "    example_q=\"What's your favorite car?\",\n",
    "    example_a=\"Tesla\",\n",
    "    input=\"What's your favorite social media site?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1e9d5-87fa-440e-a672-05f3a66afe54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f78cf0-a940-457a-8230-b35647fca25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052aeb2-5736-45fa-9b9c-0c521581baf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-preventloss-py",
   "name": "pytorch-gpu.1-13.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m105"
  },
  "kernelspec": {
   "display_name": "lglm",
   "language": "python",
   "name": "lglm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
