{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e851c63-fbb9-4f99-a92b-26dce2e719a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=./imgs/model_io.jpg width=35% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47454cdf-1217-4fbf-a015-5c981b69e2f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "[langchain documents](https://python.langchain.com/docs/modules/model_io/models/chat/llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2806292-4cf0-40eb-9f66-941d282dfab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b77b43-6e8c-4c03-b08f-ad9c273aaa96",
   "metadata": {},
   "source": [
    "[LangChain-Tutorials](https://github.com/sugarforever/LangChain-Tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f1574-f119-4e8d-bea8-96c9c06b9516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06346c2d-36d3-47dd-992b-404d25663045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-********'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83793f12-324e-4bf7-ab92-5b9f604e9e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c1835ee-a816-443c-8a03-4c6e0f41f39e",
   "metadata": {},
   "source": [
    "# MMR, Select by maximal marginal relevance\n",
    "> 最大边际相关算法, 类似于textrank simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191842ab-3e71-4d91-b7e5-0441ee79ee0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8738cd93-4db9-4315-be37-f808e7a6b760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector, SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee9fe9-13ef-40f8-bcb8-c9a3d94dda12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18e473b-c2f6-4607-88be-b945221d36af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa98d76-1040-4851-b117-57ff79df24c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# there are a lot of examples of a pretend task of creating antonyms\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a090d98-69a8-465f-ac06-7d454bf41de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7553874-9cfe-4c9e-ad81-0f1b15346d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # this is the list of examples available to select from.\n",
    "    examples=examples,\n",
    "    # this is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    # this is the vectorstore class that is used to store embedding and do a similarity search over。\n",
    "    vectorstore_cls=FAISS,\n",
    "    # this is the num of examples to preduce.\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f02f5-23cf-45c4-98b0-b73f0c5704ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395e5c1e-c534-4464-9817-7b8e7fd8217d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    # we provide an exampleselector instead of examples\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",      # 注意哦，这里规定了 末尾的 字符。\n",
    "    input_variables=['adjective']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a2bb2-50cc-4230-918b-ed4650cb8c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552d073e-f106-4795-8c57-cf091ee88ee8",
   "metadata": {},
   "source": [
    "> 输入是一个 感觉：worried， 所以第一个选择了 感觉 happy sad ， k=2， 所以还保留了一个: 多风的，和平静的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6196d79-9b56-4005-983f-bdeffdfe9c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: worried\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Input is a feeling, so should select the happy/sad example as the first one\n",
    "print(mmr_prompt.format(adjective=\"worried\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc060b84-aa03-474a-800f-b0e99617fb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5a127-8659-4229-a3dc-e9b730cf278d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffbf2428-e400-443b-82cd-21472b1e1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare this to what we would just get if we went solely off of similarity,\n",
    "# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # this is the list of examples available to select from\n",
    "    examples=examples,\n",
    "    # this is the embedding class used to produce embeddings which are used to measure semantic similarity\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    # this is the vectorstore cls that is used to store the embedding and do a similarity search over.\n",
    "    vectorstore_cls=FAISS,\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f794378-9362-41cd-a5cc-ccf3b0b07508",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # we provide ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=['adjective']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214694bc-705d-40ca-8059-24a2f7b19eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: worried\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(  similar_prompt.format(adjective='worried')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4c34d-0c8f-4bfb-b63a-1afd38d79622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d766d164-7427-434b-be64-2756bde67a48",
   "metadata": {},
   "source": [
    "> `MaxMarginalRelevanceExampleSelector` 继承了 `SemanticSimilarityExampleSelector`  <br>\n",
    "> 1. `SemanticSimilarityExampleSelector`使用了 similarity_search 相似度查找  <br>\n",
    "> 2. [MaxMarginalRelevanceExampleSelector-引用而非说明](https://arxiv.org/pdf/2211.13892.pdf) 使用了 max_marginal_relevance_search  进行相似度查找， 该算法是对similarity_search的优化."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595650c-b873-4ed9-ad0f-96f8ccb248e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8b4ac88-7fa0-45ee-9dbf-b7cab9bdb322",
   "metadata": {},
   "source": [
    ">  MMR: 它通过找到与输入具有最高余弦相似度的示例，并在此基础上进行迭代添加示例，同时对其与已选择示例的相似度进行惩罚。<br>\n",
    ">  个人理解是： 既保持了相似度， 有防止已选择示例过于雷同，而缺乏了多样性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a16283-56e6-4cae-a0f6-850389d3b505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b464ab-72e0-4c9a-96b8-45239afa7cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419c198-e0e1-4978-affd-e1f8538b55a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23625d2-1763-4adb-b746-f0c24b317066",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class SemanticSimilarityExampleSelector(BaseExampleSelector, BaseModel):\n",
    "     \"\"\"Example selector that selects examples based on SemanticSimilarity.\"\"\"\n",
    "\n",
    "     def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "         \"\"\"Select which examples to use based on semantic similarity.\"\"\"\n",
    "         # Get the docs with the highest similarity.\n",
    "         if self.input_keys:\n",
    "             input_variables = {key: input_variables[key] for key in self.input_keys}\n",
    "         query = \" \".join(sorted_values(input_variables))\n",
    "         example_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "         # Get the examples from the metadata.\n",
    "         # This assumes that examples are stored in metadata.\n",
    "         examples = [dict(e.metadata) for e in example_docs]\n",
    "         # If example keys are provided, filter examples to those keys.\n",
    "         if self.example_keys:\n",
    "             examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\n",
    "         return examples\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e3da2-9818-453a-87a9-c2ac293bc9ad",
   "metadata": {},
   "source": [
    "```python\n",
    "class MaxMarginalRelevanceExampleSelector(SemanticSimilarityExampleSelector):\n",
    "    \"\"\"ExampleSelector that selects examples based on Max Marginal Relevance.\n",
    "\n",
    "    This was shown to improve performance in this paper:\n",
    "    https://arxiv.org/pdf/2211.13892.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    fetch_k: int = 20\n",
    "    \"\"\"Number of examples to fetch to rerank.\"\"\"\n",
    "\n",
    "    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\n",
    "        \"\"\"Select which examples to use based on semantic similarity.\"\"\"\n",
    "        # Get the docs with the highest similarity.\n",
    "        if self.input_keys:\n",
    "            input_variables = {key: input_variables[key] for key in self.input_keys}\n",
    "        query = \" \".join(sorted_values(input_variables))\n",
    "        example_docs = self.vectorstore.max_marginal_relevance_search(\n",
    "            query, k=self.k, fetch_k=self.fetch_k\n",
    "        )\n",
    "        # Get the examples from the metadata.\n",
    "        # This assumes that examples are stored in metadata.\n",
    "        examples = [dict(e.metadata) for e in example_docs]\n",
    "        # If example keys are provided, filter examples to those keys.\n",
    "        if self.example_keys:\n",
    "            examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\n",
    "        return examples\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395d5b8-0529-4d91-86d3-c496163e1e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74f076-8eb2-4717-952b-b7cadb0dd8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efb44d-f4ef-41e1-9d40-330fe8a31c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b0356-9482-4dac-b0eb-ed3c6deeed39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28160f58-9769-472e-96e6-c06696b7a88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd12e48-7ac0-4c49-a3ac-85a0dcb9be57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095d68c-34e9-4602-9d65-18301bf31012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55875b-7049-42fa-a933-de1ff1233ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-preventloss-py",
   "name": "pytorch-gpu.1-13.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m105"
  },
  "kernelspec": {
   "display_name": "lglm",
   "language": "python",
   "name": "lglm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
